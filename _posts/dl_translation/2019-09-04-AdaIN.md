---
layout: post
title: "Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization"
author: "MJ"
categories: [writings, dl_translation]
tags: [translations, deeplearning, adain, paper]
image: chameleon_1280.jpg
---


## Abstract

Gatys et al. 이 발표한 Style transfer는 이미지 한장을 iterative 하게 optimize하기에 속도가 느립니다. feed-forward 뉴럴넷으로 빠르게 approximation 하는 방법이 나오면서 style transfer의 속도를 올릴 수 있게 되었습니다. 하지만 속도가 빨라진 대신 이 네트워크는 대체로 고정된 스타일 이외의 arbitrary 스타일에 대해선 적용 될 수 없었습니다.

이 논문에선 간단하면서 효과적으로 arbitrary 스타일을 (처음으로)real-time 으로 transfer 하는 방법을 소개합니다. 주요 방법은 adaptive instance normalization(AdaIN) layer로 content feature를 style feature로 normalize 하는 것입니다.

이 논문의 방법을 이용하면 유저가 content-style trade-off, style interpolation, color & spatial controls 등을 하나의 feed-forward neural network로 컨트롤 할수 있다고 합니다.

---

## Background

### Batch Normalization

Batch Normalization(BN) layer는 뉴럴 네트워크를 학습시킬때 학습에 도움을 주는 테크닉 으로 알려져 있습니다. BN은 feature statistics를 normalize 하는것으로 image generating modeling에도 효과가 있다고 알려져 있습니다.

input batch 가 다음과 같을때,

$$x \in \mathbb{R}^{N \times C \times H \times W}$$

(N : batch size, C : channel, H : 이미지 높이, W : 이미지 너비)

BN은 각각의 feature channel에 대해:

$$\mathrm{BN}(x)=\gamma\left(\frac{x-\mu(x)}{\sigma(x)}\right)+\beta$$

평균과 표준표차로 normalize 합니다. (원문은 BN normalizes the mean and standard deviation for each individual feature channel:

- 원문

3. 여기서 감마와 베타는 각각,

$$\gamma, \beta \in \mathbb{R}^{C}$$

data로 부터 학습하는 affine parameters 입니다.

그리고, 평균과 표준표차는 

각 feature channel과 무관하고, 배치 사이즈(N)와 spatial dimension(HxW)에 대해 계산된 값입니다.

$$\mu(x), \sigma(x) \in \mathbb{R}^{C}$$

$$\mu_{c}(x)=\frac1{NHW}\Sigma_{n=1}^N\Sigma_{h=1}^H\Sigma_{w=1}^Wx_{nchw}$$

$$\sigma_{c}(x)=\sqrt{\frac1{NHW}\Sigma_{n=1}^N\Sigma_{h=1}^H\Sigma_{w=1}^W(x_{nchw}-\mu_{c}(x))^2+\epsilon}$$

** BN은 학습 도중에 mini-batch 통계량을 이용하고, 추론 도중에 그를 popular statistics으로 대체합니다.  (이것으로 '훈련'과 inference의 모순을 알리며.) 

** batch renormalization작업은 훈련에서 점진적으로 popular statistics를 쓰며 이 문제를 해결하려 한 것입니다.

BN의 또다른 흥미로운 응용은 Li et al.이 발견한 것으로,

** BN이 타겟 도메인의 popular statistics을 다시 계산하여 'domain shift' 문제를 완화시킨다는 것이었습니다.

### Instance Normalization

---

기존 feed-forward 방식의 stylization 기법에서 

스타일 트랜스퍼 네트워크는 각 컨볼루션 층 다음에 BN 층을 놓습니다. 

그런데 놀랍게도, BN 층을 Instance Normalization (IN) 층으로 바꾸는 것만으로도 

성능이 상당히 향상되는 것을 Ulyanov et al.이 발견합니다.

$$\mathrm{IN}(x)=\gamma\left(\frac{x-\mu(x)}{\sigma(x)}\right)+\beta$$

BN과의 차이점은

이때 이용되는 평균과 표준편차가 channel과 샘플과는 무관하고, spatial dimension (HxW) 에 대해 계산된 통계량이라는 점입니다.

$$\mu_{nc}(x)=\frac1{HW}\Sigma_{h=1}^H\Sigma_{w=1}^Wx_{nchw}$$

$$\sigma_{nc}(x)=\sqrt{\frac1{HW}\Sigma_{h=1}^H\Sigma_{w=1}^W(x_{nchw}-\mu_{nc}(x))^2+\epsilon}$$

IN과 BN의 또다른 차이점은 

** IN 층은 are applied at test time unchanged,

** BN 층은 보통 미니배치 통계량을 모수 통계량(population statistics)으로 대체합니다.(replace)

[](https://www.notion.so/87bd3ffecab54d84bf8d047be1dbd4c6#2e5405fd24c749e1aa2e69a80d8b4bf4)

출처: [https://blog.lunit.io/2018/04/12/group-normalization/](https://blog.lunit.io/2018/04/12/group-normalization/)

### Conditional Instance Normalization

---

한 쌍의 affine parameter 감마와 베타를 학습하는 대신, Dumoulin et al.은 

각 스타일 s마다 서로 다른 쌍의 parameter 감마와 베타를 학습시킨 

Conditional Instance Normalization (CIN) 층을 제안합니다. 

$$\mathrm{CIN}(x)=\gamma^s\left(\frac{x-\mu(x)}{\sigma(x)}\right)+\beta^s$$

위의 식이 스타일 트랜스퍼 네트워크에서 적용되는 과정은 다음과 같습니다.

1. 우선 훈련용 스타일 이미지는 스타일 s를 가지며, s는 고정된 스타일 집합의 임의의 원소입니다.

$$s \in \{1, 2, ..., S\}\ \ \ \ \ \ \ \ \ (그들의\ 실험에서\ S=32)$$

2. 각 스타일 s에 대해 parameter 감마와 베타를 훈련합니다. 

3. 이후 콘텐츠 이미지가 스타일 트랜스퍼 네트워크를 통과하며, 여기서 CIN 층을 사용할 때 입히고자 하는 스타일에 대응되는 감마와 베타를 이용합니다.  

- 원문

4. 놀랍게도 네트워크는 같은 컨볼루션 parameter를 이용하면서도 

IN층에서 다른 affine parameter를 사용함으로써, 완전히 다른 스타일의 이미지를 생성할 수 있게 되었습니다. 

하지만 CIN 층으로 정규화하는 방식에도 한계는 있습니다.

정규화를 시키는 층이 없는 네트워크와 비교했을 때, 

CIN 층이 있는 네트워크는 2FS 개의 추가적인 파라미터를 요합니다. (F: 피처맵의 총 개수)

추가로 필요한 파라미터의 개수가 스타일의 개수에 대해 선형적으로 늘어나므로 (scales linearly)

스타일의 개수가 많을 때 적용하기 어렵습니다. (예를 들어 수 만개일 때)

또한 이러한 접근으로는 

네트워크를 재학습시키지 않고는 임의의 새로운(arbitrary new) 스타일에 대해 적용할 수 없습니다. 

### Interpreting Instance Normalization

---

본 논문은 콘텐츠 이미지에 대비해 불변하는 것 이외에 IN의 또다른 성공 요인을 찾고자 했습니다.  

IN이 feature space 내에서 이뤄지기 때문에 

단순히 pixel space에서의 분포를 대비 표준화(contrast normalization)시키는 것보다 더 심오한 영향을 줄 것이라고 생각했기 때문입니다. 

본 논문은 IN의 affine parameter를 출력 이미지의 스타일을 완전히 바꿀 수 있는 도구로 가정합니다.

구체적으로,

IN이 피처 통계량의 분포를 정규화하여 스타일의 분포를 정규화하는 역할을 수행한다고 가정합니다.

판단하게 된 근거는 아래의 연구로부터 입니다.

- 본래 DNN의 convolutional feature statistics는 이미지의 스타일을 추출할 수 있다고 알려져 있습니다.
- 한편 Gatys et al. 은 최적화 목표로 2차 통계량을 사용했고, (use the second-order statistics as their optimization objective)
- Li et al.은 채널 단위의(channel-wise) 평균과 분산을 포함해 다른 여러 통계량을 match 하면 스타일 트랜스퍼에 용이하다는 것을 보였습니다.
- 즉, 전통적으로는 DNN이 이미지에 대해 이야기해주는 역할을 하지만, 생성기 네트워크의 피처 통계량 역시 생성될 이미지의 스타일에 대한 통제가 가능하다고 판단했습니다.

본 논문은 위의 가정을 확인하기 위해 3개의 텍스처 네트워크를 비교하고, 이는 IN 층과 BN 층을 이용하는 single 스타일 트랜스퍼를 수행합니다. 결과로,

1. MS-COCO의 원본 이미지로 학습 시 IN을 이용할 때가 BN보다 수렴 속도가 빠릅니다.
2. 다음으로, 훈련이미지의 luminance(밝기) 채널에 대해 히스토그램 평활화(histogram equalization)을 실시해 비교 가능한 범위 내로 조정합니다. (normalize all the training images to the same contrast) 이때, IN 층을 넣은 모델은 여전히 BN 층보다 효과적입니다.
3. 마지막으로, 사전 학습된 스타일 트랜스퍼 네트워크로 훈련이미지를 모두 같은 스타일에 대해(타겟 이미지와는 다름) 정규화 합니다. 

    그러자 IN 층을 넣은 모델과 BN층이 있는 모델의 성능 차이가 1, 2 보다 작았습니다. 또한 BN 층이 있는 모델의 수렴 속도도, 원본 이미지가 IN 층이 있는 모델로 학습될 때만큼 빠릅니다.

(BN 층과의 성능 차이가 여전히 존재하기는 하는데 본 논문은 이를 훈련이미지에 대한 스타일 정규화가 완벽하지 않았기 때문이라고 해석합니다.)

실험 결과, 본 논문은 IN 이 스타일의 분포를 일종의 정규화시키는 역할을 한다고 해석합니다. 

BN은 미니 배치 속 샘플이 하나의 스타일에 대해 균일한 분포를 띄도록 정규화한다고 이해할 수 있습니다. 

그러나 각각의 샘플은 서로 다른 스타일을 가질 수도 있습니다. 

따라서 모든 이미지를 하나의 스타일로 바꿀 때 바람직하지 않습니다.

컨볼루션 층은 배치 내부의 스타일 차이를 보강할 수 있는 학습을 하겠지만, 이것은 훈련에 대해 추가적인 작업을 요합니다. 

그에 반해 IN은 각의 샘플의 스타일을 타겟 스타일에 대해 정규화시킬 수 있습니다. 

그러면 네트워크는 원본 스타일 정보는 내버려두고 콘텐츠 제어에만 집중하므로 훈련이 쉬워집니다.

그러면 CIN의 성공에 대한 이유도 조금 명확해집니다. 

서로 다른 affine parameter가 피처 통계량을 정규화시키면서 분포를 다르게 만들었고, 따라서 출력 이미지를 서로 다른 스타일에 대해 정규화할 수 있기 때문입니다.

### Adaptive Instance Normalization

---

$$\mathrm{AdaIN}(x)=\sigma(y)\left(\frac{x-\mu(x)}{\sigma(x)}\right)+\mu(y)$$

콘텐츠 입력값 x와 스타일 입력값 y가 있을 때 

x의 분포를 표준화한 후 y의 채널 단위 평균과 분산으로 맞춥니다. 

위 정규화와의 차이는 affine parameter가 학습될 필요가 없다는 점입니다. 대신 스타일 입력값이 들어올 때마다 affine parameter를 계산합니다. ( adaptively ) 

AdaIN이라는 정규화는 IN과 마찬가지로 spatial location에서 계산됩니다.

AdaIN은 feature space 내에서 피처 통계량(특히 채널 단위의 평균, 분산)을 전달함으로써 스타일 트랜스퍼의 역할을 수행합니다.

AdaIN은 IN층만큼 간단하고 계산비용이 적게 든다고 합니다.

---

## Experimental Setup

### Architecture

---

[](https://www.notion.so/87bd3ffecab54d84bf8d047be1dbd4c6#1c7d551f56f741de8b682c9a71b8509f)

T: 스타일 트랜스퍼 네트워크   /   T(c,s): c의 콘텐츠와 s의 스타일이 합성된 이미지

s: 스타일 이미지   /   c: 콘텐츠 이미지   /   f: 인코더   /   g: 디코더   /   t: 타겟 피처맵

1. T는 c와 임의의 s를 입력으로 받고 c의 콘텐츠와 s의 스타일이 합성된 이미지를 출력합니다.
2. T는 f-g의 아키텍처를 가지며, 인코더 f는 사전학습된 VGG-19의 앞 일부 층을 사용합니다.
3. c와 s를 feature space로 인코딩한 후 두 개의 피처맵을 AdaIN 층으로 전달합니다.
4. AdaIN 층에서 c의 피처맵이 s의 피처맵의 평균값과 분산값으로 정규화되도록 맞춥니다.
5. AdaIN 층에서 타겟 피처맵 t를 출력합니다.   t = AdaIN (f(c), f(s))
6. 랜덤하게 초기화된 상태에서 디코더 g의 훈련: t가 image space로 환원돼 T(c,s)를 생성하도록 훈련합니다.   T(c,s) = g(t)

아키텍처를 구성할 때 고려한 점은 다음과 같습니다.

디코더 g는 거의 인코더 f를 뒤집은 모습입니다. 다만 풀링 층은 가장 가까운 업샘플링 층으로 대체되어 checker board 효과를 방지합니다. f와 g에는 reflection padding을 이용해 border artifacts를 피합니다. 

디코더 g에는 instance또는 batch에 대한 정규화 작업을 하는 층을 넣지 않습니다. IN 층이 각 샘플 하나의 스타일에 대해 정규화시키거나  BN 층이 미니 배치 속 샘플들을 하나의 스타일에 대해 정규분포하게 하는 것은 바람직하지 않기 때문입니다. 

## Training

---

데이터셋:   콘텐츠 이미지 - MS-COCO | 스타일 이미지 - WikiArt의 8만 장의 그림

최적화 알고리즘: Adam

배치 사이즈: 8쌍의 콘텐츠와 스타일 이미지

training: 

- 학습 시에 이미지는 가장 작은 512 사이즈로 조정, 가로세로비(aspect ratio)는 보존하고 256x256 공간을 무작위로 크롭합니다.
- 사전학습된 VGG-19를 이용한 디코더 g를 훈련하기 위한 손실함수 :

$$L=L_c+\lambda{L_s}$$

- 콘텐츠 loss는 타겟 피처와 출력 이미지 피처의 유클리드 거리입니다. 콘텐츠 타겟으로는 AdaIN 층의 출력인 t를 이용합니다. :

$$L_c=\Vert{f(g(t))-t}\Vert_2$$

- 스타일 loss를 이용해 일반적으로 이용되는 Gram matrix loss가 아닌, IN 통계량을 맞춥니다. AdaIN 층은 스타일 피처의 평균과 표준편차만 전달하는 컨셉이기 때문입니다. :

$$L_s=\Sigma_{i=1}^L\Vert{\mu(\phi_i(g(t))-\mu(\phi_i(s))}\Vert_2 \\+ \Sigma_{i=1}^L\Vert{\sigma(\phi_i(g(t))-\sigma(\phi_i(s))}\Vert_2 $$

** 이때 함수 phi_i는 VGG-19에서 스타일 loss를 계산하는 층을 나타냅니다. 본 논문에서는 relu1_1, relu2_1, relu3_1, relu4_1층을 동일한 가중치와 함께 이용했습니다. 

---

## Results

[](https://www.notion.so/87bd3ffecab54d84bf8d047be1dbd4c6#b1b2d6786a974ef49629970ed8ada562)

[](https://www.notion.so/87bd3ffecab54d84bf8d047be1dbd4c6#90308169cb294880b1fc68c0cd15263a)

** : 잘 모르겠는 부분

###### Reference

[https://blog.lunit.io/2018/04/12/group-normalization/](https://blog.lunit.io/2018/04/12/group-normalization/)


