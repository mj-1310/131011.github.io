---
layout: post
title: "머신러닝 15. 지도학습: 다중공선성 해소를 위한 정규화&축소 모델"
author: "MJ"
categories: [science, statistical_method]
tags: [statistics, machinelearning, multicampus, bigdata_analysis_edu, CRT]
image: 
---

---
&nbsp; &nbsp; **목차**<br>
&nbsp; &nbsp; 1. [패키지 로딩 & 데이터셋 로딩](#1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [패키지 일괄 설치&로딩 사용자 정의 함수 설정 후 시행](#1_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [데이터셋 로딩](#1_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [데이터셋 분할 및 간단조회: initial_split() / str() / skimr::skim()](#1_3)<br>
&nbsp; &nbsp; 2. [데이터탐색: 다중공선성에 대한 의심을 중심으로](#2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [선형회귀분석 실시: stats::lm()](#2_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [2개 예측변수 간 상관성 파악: cor()](#2_2)<br>
&nbsp; &nbsp; 3. [정규화 회귀분석](#3)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [훈련/테스트데이터셋 모델링 후 구조 파악: model.matrix()](#3_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [릿지모델에 훈련데이터 피팅: glmnet(alpha = 0) / plot()](#3_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [릿지모델 피팅 결과 확인: $lambda / coef()](#3_3)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (4) [릿지모델에 훈련데이터 적용 후 교차검증: cv.glmnet() / plot()](#3_4)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (5) [릿지모델 성능 평가(MSE) 및 변수 중요도 파악](#3_5)<br>
&nbsp; &nbsp; 4. [축소 회귀분석](#4)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [라쏘모델에 훈련데이터 피팅: glmnet(alpha = 1) / plot()](4_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [라쏘모델 피팅 결과 확인: cv.glmnet() / plot()](#4_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [라쏘 성능 평가(MSE) 및 변수 중요도 파악](#4_3)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (4) [릿지모델과 라쏘모델 간 성능 비교](#4_4)<br>
&nbsp; &nbsp; 5. [엘라스틱 넷](#5)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [라쏘모델에 훈련데이터 피팅: glmnet(alpha = 0~1) / plot()](#5_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [모델간 비교: plot()](#5_2)<br>


---


<br>
023

### 1. 일반회귀분석의 가정조건: 다중공선성(Multicollinearity) 문제


##### 다중공선성이 있는 경우 --> 예측변수가 늘어날 수록 variance가 커지는 문제발생

- 고전적인 선형회귀모델에서는 회귀계수 추정을 위해 잔차의 제곱합을 계산하는 일종의 비용함수를 만들어서 비용함수가 최소가 되는 회귀계수를 추정하게 됨

- 하지만 다중공선성이 있는 독립변수들이 투입된 상태의 비용함수에서는 회귀계수의 영향력이 과다추정될 수 있음

- 분산팽창지수 vif(Variance Inflation Factor) 값이 10 이상이면 독립변수들간 강한 상관관계가 존재해 종속변수의 수치예측에 영향을 미침

- 다중 공선성은 모형의 일부 예측 변수가 다른 예측 변수와 상관되어 있을 때 발생하는 조건으로 회귀계수의 분산을 증가시켜 과다추정 계수를 만듬

- 예측변수와 반응변수 간에 유의한 관계가 있음에도 없는 것처럼 보이게 함

- 높은 상관 관계가 있는 예측변수의 계수 부호가 반대방향으로 나타나게 함





### 2. [A] 정규화(regularization)와 축소(Shrinkage) 회귀모델



- 일반적 선형회귀계수에 대한 제약조건/벌점/페널티을 추가함으로써 과최적화를 막는 방법으로 Regularized Method, Penalized Method, Contrained Least Squares라고도 함

- 고전적인 선형회귀모델의 회귀계수 추정시 사용하는 잔차의 제곱의 합을 계산하는 비용함수에 페널티(regularization) 함수라는 추가적인 수식을 붙여 회귀계수값들의 과다추정을 막고, 오버피팅도 방지함

- 기존의 잔차제곱합을 계산하는 비용함수에 페널티를 어떻게 줄 것인지에 따라 여러 개의 세부적인 회귀분석으로 나누어짐

- 데이터의 갯수에 비해 독립변수의 개수가 많을 때도 이런방식을 사용할 수 있음

- 릿지회귀모델(정규화)은 벌점을 부과해 중요도가 낮은 피처의 회귀계수를 거의 0에 가까운 수치로 만들어 줌 

- 라소회귀모델(축소)은 벌점을 부과해 중요도가 낮은 피처의 회귀계수를 0으로 만들어 줌으로써 자동화된 피처선택을 수행하고 예측력을 높여줌

- 엘라스틱넷모델은 릿지모델의 페널티와 라소모델의 패털티를 결합해서 부과하는 모델임


##### 릿지;능형(Ridge) 회귀모델

- 회귀계수 추정에 따른 잔차의 제곱합을 최소화하는 계산방식에다가 --> 회귀계수의 제곱합(squared sum of weights)을 최소화하는 제약조건 추가

- Ridge 모형은 예측변수들의 회귀계수를 한꺼번에 축소시키면서 가중치가 클 수록 큰 페널티를 부과해 오버피팅을 억제하는 효과가 있는데, 이를 L2 regularization(벌점;페널티) 특성을 가짐

- 능형회귀는 변수가 많아 높은 분산을 가지는 상황에서 계수의 크기가 거의 동일한 크기일 때 성능이 좋음

- 정규화 된 회귀는 계수의 크기에 대한 제약을 가하고 점차적으로 0으로 축소하는 방식사용

- 제약 조건은 계수의 크기와 변동을 줄이는 데 도움이되며 모델의 분산을 줄여 예측력의 일관성을 유지해 줌

###### 정규화 회귀모델의 조율파라미터(tuning parameter): lambdas(람다)

- 회귀모델의 각 피처별 회귀계수의 과다추정을 막기위해서 수축패널티를 0에서 무한까지 조절하면 예측모델의 편향과 분산값 사이의 절충(trade-off)이 발생함 

- 이 둘의 합인 검정MSE가 최소인 지점을 찾게 되고 이 때의 계수 추정치를 구해 줌


● 회귀계수 튀는 부분을 잡아 냄. 변수 개수 그대로 남는다.

● 독립변수 결합해서 다중공선성 있는 것 찾아내서 잡아내는 것.


##### 라쏘(LASSO) 회귀모델(Least Absolute Shrinkage and Selection Operator) 

- 회귀계수 추정에 따른 잔차의 제곱합을 최소화하는 계산방식에다가 --> 회귀계수의 절대값의 합을 최소화하는 제약조건 추가

- Lasso 모형은 일부 예측변수의 회귀계수가 먼저 0으로 수렴하는 특성을 가짐

- 작은 가중치(회귀계수)들은 거의 0으로 수렴하게 되며, 몇 개의 중요한 가중치만 남게되므로 피처선택(feature selection) 효과가 있는데, 이를 L1 regularization(벌점;페널티)이라고 함

- 적은 수의 설명변수가 상당히 큰 계수를 가질때 잘 작동함


● 변수의 수 줄임. 피처 셀렉션 되는 기법.

● 라쏘는 올가미라는 뜻.



##### 엘라스틱넷(Elastic Net) 회귀모델

- Lasso와 Ridge의 제약조건을 결합한 모형으로 가중치의 절대값의 합과 제곱합을 동시에 최소화하는 제약조건을 가짐


● 기존의 회귀계수 구하는 공식 + 릿지와 라쏘의 방식 모두 집어넣음.

● 피처셀렉션하면서 전반적으로 데이터 줄여나가는 기법 씀.



### 3. [B] 데이터 축소(Data reduction) 방식

● 그룹핑, 유형과 등의 GNT에서 배운 기법

##### PCR(Principal Component Regression)

- 독립변수들의 주성분을 추출/이용해 회귀모델을 만듬

- 주성분들이 서로 직교하므로 다중공선성 발생하지 않음

- 상위 몇 개 주성분만 이용할 경우 라쏘처럼 일종의 regualization 효과를 발생시켜 모델이 오버피팅 현상도 완화됨

- 그러나 모델 해석은 어려울 수 있음


##### PLS(Partial Least Square) regression

- PCR과 비교했을 때 변수변환 방식에서 차이가 남

- PCR: 독립변수의 분산을 최대로 하는 축을 찾아 데이터를 전사(projection)하는 방식으로 독립변수만 변형함

- PLS: 종속변수와 독립변수의 관계를 가장 잘 설명해주는 축을 찾아 전사하는 방식으로 종속변수와 독립변수 모두를 변형함


<br>

---------------코딩------------------------------------------------------------------------------------------------------

<br>

<a id="1"></a>

## 1. 패키지 로딩 & 데이터셋 로딩

<br>

<a id="1_1"></a>

### (1) 패키지 일괄 설치&로딩 사용자 정의 함수 설정 후 시행

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="1_2"></a>

### (2) 데이터셋 로딩

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="1_3"></a>

### (3) 데이터셋 분할 및 간단조회: initial_split() / str() / skimr::skim()

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2"></a>

## 2. 데이터탐색: 다중공선성에 대한 의심을 중심으로

<br>

<a id="2_1"></a>

### (1) 선형회귀분석 실시: stats::lm()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_2"></a>

### (2) 2개 예측변수 간 상관성 파악: cor()

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3"></a>

## 3. 정규화 회귀분석

<br>

<a id="3_1"></a>

### (1) 훈련/테스트데이터셋 모델링 후 구조 파악: model.matrix()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_2"></a>

### (2) 릿지모델에 훈련데이터 피팅: glmnet(alpha = 0) / plot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_3"></a>

### (3) 릿지모델 피팅 결과 확인: $lambda / coef()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_4"></a>

### (4) 릿지모델에 훈련데이터 적용 후 교차검증: cv.glmnet() / plot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_5"></a>

### (5) 릿지모델 성능 평가(MSE) 및 변수 중요도 파악

<br>

<a id="4"></a>

## 4. 축소 회귀분석

<br>

<a id="4_1"></a>

### (1) 라쏘모델에 훈련데이터 피팅: glmnet(alpha = 1) / plot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4_2"></a>

### (2) 라쏘모델 피팅 결과 확인: cv.glmnet() / plot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4_3"></a>

### (3) 라쏘 성능 평가(MSE) 및 변수 중요도 파악

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4_4"></a>

### (4) 릿지모델과 라쏘모델 간 성능 비교

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="5"></a>

## 5. 엘라스틱 넷

<br>

<a id="5_1"></a>

### (1) 라쏘모델에 훈련데이터 피팅: glmnet(alpha = 0-1) / plot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="5_2"></a>

### (2) 모델간 비교: plot()


<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

###### <참고 문헌> 

<br>

1. 최점기 박사님 강의 <br>
