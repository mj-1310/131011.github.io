---
layout: post
title: "머신러닝 16. 지도학습: 비선형 모형"
author: "MJ"
categories: [science, statistical_method]
tags: [statistics, machinelearning, multicampus, bigdata_analysis_edu, GNT]
image: 
---


## 1.  수치예측(회귀분석) 중 비선형회귀분석: 다항회귀모형



### 1-1. 일반회귀분석의 가정조건: 선형성(Linearity)

- 투입/예측/설명/독립변수와 산출/결과/반응/종속변수 관계가 선형적인지 확인

- 개별 투입변수가 종속변수와 상관성이 있는지 상관관계 그래프(scatter matrix plot)를 그려서 상관계수 정도와 선형적 직선의 모양/방향 확인

- 독립변수들과 종속변수 간에 선형관계가 있다면 예측치(Fitted values)와 잔차(Residuals) 특별한 관련성이 보이지 않아야 함

- 잔차들의 평균0, 분산이 일정하면 선형성 만족함




● 구간에 따라 잔차가 어느 방향으로 쏠리니 다항 회귀를 써야겠다는 판단이 나옴.



### 1-2. 독립변수와 종속변수간 선형관계가 아닌경우의 대안

- 곡선회귀(Curvilinear regression); 비선형회귀(Nonlinear regression)



● 예측선이 하나의 직선으로 나오지 않으므로 새로운 그래프 모양을 찾은게 비선형회귀.
y = a x x^2 + b 나, 거기서도 예측선이 잔차를 품지 못하면 삼차식, 사차식의 곡선의 예측선을 만든다. 이걸 다항회귀라고 한다. 

● 수학에서 x에 제곱을 씌우니까 그래프의 곡선 모양이 나오던 거에서 착안한 것. 그래서 독립변수에 제곱을 씌워서 원하는 그래프를 만드는 것. 



(1) 다항(Polynominal) 회귀분석

- 독립변수가 다항식으로 구성되는 회귀모델로 변경

- 독립변수에 지수승을 붙여서 여러 개의 변수로 만들의 회귀모델을 구성하는 기법

- I(x ** 2) 또는 I(x ^ 2)와 같은 형태의 수식을 사용하여 다항회귀모델을 표현함

- 보통 4차이상의 모형은 잘 사용하지 않음


● I는 하나의 약속. 다항회귀를 만들 때 이런 수식으로 독립변수를 가공해 달라는 약속.



(2) 스플라인회귀분석(Spline Regression)

- 다항회귀의 차수를 높이는 방식은 예상치 못한 이상한 회귀함수 모양이 나타날 가능성이 있음

- 회귀스플라인은 매듭(knot)으로 나누어진 일정한 구간마다 낮은 차원의 다항식 모형을 적합하는 방식을 채택함

- 조각별 다항식(piecewise polynominal)이라고도 함


● 스플라인이라는 건 곡선이라는 뜻.

● x축 구간을 단계별로 쪼개서 이 구간 정도의 x가 투입되었을 때의 y값 예측하는 예측선을 그리고, 다음 구간의 x에 대한 예측선을 구하는 것.

● 컴퓨터는 직선 밖에 그리지 못하지만, 부드러운 벡터 곡선을 그리기 위한 기법.



(3) 일반화가법모형(Genelizaed Additive Model)

- 각 독립변수별로 종속변화와의 관계를 나타내는 개별적인 비선형 예측식을 만들고 이들 간의 선형적 결합을 통해 예측성능을 향상시

- 다양한 함수를 사용하면서 다양한 관계를 표시할 수 있으므로 일반화(generalized)라는 이름이 붙음


● 독립변수 별로 변수의 특성이 다를 것이다. 그러므로 개별 변수의 특성에 따라 어떤 건 제곱을 통해 집어넣기도 하고, 어떤 변수는 스플라인 방식으로 설정해서 서로서로 더해서 예측하는 모형. 1번 독립변수는 이방식, 2번 독립변수는 저 방식.

● generalized: 일반화라는 건 lm함수의 조건이 정교하게 맞지 않다보니, 일반화, 조건을 회피할 수 있는 방식을 사용한 것.


## 2. 일반회귀분석의 가정조건: 이상치/특이점 문제

- 보통 일반 회귀분석은 예측식을 구성하는 독립변수들의 예측계수(회귀계수)를 구할 때 잔차의 제곱의 합이 최소과 되게하는 최소제곱법(Method of Ordinary Least Squares, OLS) 방법을 사용함

- 잔차의 합=0이므로 제곱의 합이 최소가 되는 방법을 사용함

- OLS로 회귀선을 추정하는 과정에서 특이점이 존재할 경우 이 특이점은 회귀직선의 결정에 큰 영향을 미치게 됨


### (1) 로버스트 회귀(robust regresion)

- OLS를 적용하는 모든 예측상황에서 사용가능하며, 가중치 메커니즘을 사용해 영향력 있는 관측치를 낮춰줌

- 특히 데이터에서 이상치를 제외시킬 강력한 이유가 없는 경우에 유용함

- 잔차의 제곱을 이용하는 최소제곱법 대신에 절대값의 합이 최소가 되도록 계수룰 추정하는 방식

● 절댓값을 이용.


### (2) 분위수 회귀(quantile regresion)

- 평균이 아닌 특정 분위값을 추정해서 그 위치에 있는 종속변수값을 사용해 아웃라이어의 영향을 해소

● 분위수를 구하면 단계별로 x가 변할 때 대충 y가 어느 위치인지 예상 가능. y축에 아웃라이어 있으니, x의 분위수를 나눠 상대적으로 아웃라이어에 대한 민감성을 낮춤.
