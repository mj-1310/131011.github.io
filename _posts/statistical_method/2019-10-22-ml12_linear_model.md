---
layout: post
title: "머신러닝 12. 지도학습: 선형모형"
author: "MJ"
categories: [science, statistical_method]
tags: [statistics, machinelearning, multicampus, bigdata_analysis_edu, GNT]
image: 
---

---
&nbsp; &nbsp; **목차**<br>
&nbsp; &nbsp; 1. [데이터셋 로딩](#1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [데이터셋 로딩](#1_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [데이터셋 간단조회: head() / str()](#1_2)<br>
&nbsp; &nbsp; 2. [데이터탐색](#2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [기술통계: summary()](#2_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [산점도: plot() / PerformanceAnalytics::chart.Correlation()](#2_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [선형성 가정: plot() / crPlots()](#2_3)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (4) [독립성 가정: durbinWatsonTest()](#2_4)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (5) [정규성 가정: plot()](#2_5)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (6) [등분산성 가정: plot() / ncvTest() / spreadLevelPlot()](#2_6)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (7) [영향 관측치와 이상치 파악: plot() / car::outlierTest() / influencePlot()](#2_7)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (8) [다중공선성 가정: vif() > 10](#2_8)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (9) [한꺼번에 진단: plot() / gvlma()](#2_8)<br>
&nbsp; &nbsp; 3. [데이터 전처리](#3)<br>  : 021 ㅡ022
&nbsp; &nbsp;&nbsp;&nbsp; (1) [종속변수 지정](#3_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [독립변수 중 범주형 변수컬럼 지정](#3_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [독립변수 중 연속형 변수컬럼 지정: setdiff()](#3_3)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (4) [독립변수 중 범주형 변수컬럼을 일괄팩터화하는 사용자 정의함수 및 일괄팩터화](#3_4)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (5) [독립변수 중 연속형 변수컬럼을 일괄스케일링하는 사용자 정의함수 및 일괄스케일링](#3_5)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (6) [전체 데이터를 6:2:2로 분할: 학습, 검증, 테스트](#3_6)<br>
&nbsp; &nbsp; 4. [훈련데이터 다중회귀분석](#4)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [훈련데이터로 다중회귀모델 설정: stats::lm()](#4_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [회귀분석결과 확인: summary() / confint() / signif() / coef()](#4_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [회귀예측모델 독립변수 선택 방법: stats::step() / MASS::stepAIC()](#4_3)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (4) [회귀분석에 사용된 투입변수 중요도 계산: varImp() / ggplot()](#4_4)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (5) [훈련데이터로 학습한 모델간 평가: AIC() / BIC() / summary()$adj.r.squared](#4_4)<br>
&nbsp; &nbsp; 5. [검증데이터&테스트데이터 수치 예측](#5)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [데이터 예측: predict(모델, 데이터, interval = "")](#5_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [검증데이터 대상 회귀모델간 수치예측 성능평가: caret::defaultSummary() / Metrics::rmse() / Metrics::mae()](#5_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [테스트셋을 대상으로 최종적으로 모델간 성능평가: 예측 및 성능평가](#5_3)<br>
&nbsp; &nbsp; 6. [예측 결과 정리](#6)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (1) [예측결과 결합: cbind()](#6_1)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (2) [예측결과 내림차순 정리: decreasing = T](#6_2)<br>
&nbsp; &nbsp;&nbsp;&nbsp; (3) [예측값 필터링](#6_3)<br>
&nbsp; &nbsp; 7. [회귀분석 모델 진단 그래프: 멀티캔버스 / 개별 출력](#7)<br>

---


<br>

● 회귀분석에는 상관계수를 응용해 회귀계수 이용.

● 독립변수 늘수록 회귀분석의 정확도 높아짐.

● 예측력이 생기려면, 독립변수와 회귀변수가 어느 정도 상관관계가 있어야 한다.

● 또 중요한 거 하나는 독립변수 간의 관계. 독립변수도 상관성이 높은 변수들이 있다. 이건 다중공선성의 위험이 있을 수 있다. 얘는 주성분분석이나 요인분석에 의해 묶어주는 것도 하나의 방법이다. 

---


## 기본 용어 이해

### (1) 오차(error)

- 전체 데이터를 활용한 예측모델에서 추정된 값과 실제값의 차이


### (2) 잔차(residual)

- 표본 데이터를 활용한 예측모델에서 추정된 값과 실제값의 차이


### (3) 편차(deviation)

- 관측치가 평균으로부터 떨어져 있는 정도


## 회귀분석 모델진단 

### (1) 선형성(Linearity) 가정: 독립변수와 종속변수간 선형관계인지 파악 -> X -> 다항회귀(Polynomial) / 일반화가법모형(GAM)

- 투입/예측/설명/독립변수와 산출/결과/반응/종속변수 관계가 선형적인지 확인

- 개별 투입변수가 종속변수와 상관성이 있는지 상관관계 그래프(scatter matrix plot)를 그려서 상관계수 정도와 선형적 직선의 모양/방향 확인

● 선형성 가정은 논리적으로 독립변수가 종속변수의 원인 변수라는 논리성이 있어야 하며, 상관성의 측면에서 서로 상관관계가 있어야 한다는 전제가 있어야 함. 

근데 실제 그래프는 잔차에 대한 그래프. 

● 최소자승기법: 실제 데이터와 예측값의 차이를 제곱해 더했을 때 그 양적인 값이 최소가 되는 지점을 찾아서 직선을 그리고 그 기울기를 구하는 기법.  

● 선형회귀는 최소자승기법을 전제로 함.

● 결국 이런 잔차를 모았을 때 큰 잔차와 작은 잔차를 모두 모아보면, 일정한 패턴 없이 거의 무작위성으로 잔차가 존재해야 한다는 뜻. 여기이 이렇게 되면 선형성이 있다고 본다. 

● 잔차가 평균 0에 가까울 수록 선형성이 있다.

### (2) 독립변수와 종속변수간 선형관계가 아닌경우

##### 다항회귀(Polynominal)

- 독립변수가 다항식으로 구성되는 회귀모델로 변경

- 독립변수에 지수승을 붙여서 여러 개의 변수로 만들의 회귀모델을 구성하는 기법

● 아까와는 달리 독립변수를 제곱, 세제곱해서 예측선을 꺾이게 만듦.


##### 일반화가법모형(GAM)

- 독립변수를 그대로 이용하는 것이 아니라 다른함수의 선형결합으로 표현

- 다양한 함수를 사용하면서 다양한 관계를 표시할 수 있으므로 일반화(generalized)라는 이름이 붙음 


### (3) 독립성(Independence) 가정: 오차항에 시계열적 특성이 있는지 파악 -> X -> 자기회귀(ARMA / ARCH)

- 투입/예측/독립 변수들간에 아무런 관련성이 없어야 함

- 무작위 샘플링을 통해서 데이터가 시간적/공간적/조건적 상황에 영향을 받지 않고 수집되어야 함

● 독립성이란 건, 시계열 개념이 있냐 없느냐의 의미. 이전 데이터의 영향을 받아 나오는게 아니라, 시간과 공간에 상관없이, 횡단면, 즉 동일한 데이터 특성으로 측정된 것. 

● 데이터가 일정 시간에 따라 측정된 것이기 때문에, 시계열적 조건에 따라 달라질 수 있으니, 시간에 상관없이, 조건에 상관없이, 독립적인 변화패턴이 일관적인 게 있는지 따지는 기법. 

● 그래서 얘를 자기상관성이라고 부른다. 시계열 데이터가 자기상관적으로 움직인다. 주가같은 것. 그런 건 독립성 가정 하면 안돼. 

### (4) 오차항에 자기상관성이 있는 경우

##### 자기회귀(Autoregressive Model)

- 보통의 시계열 데이터와 같이 순서가 있는 데이터인 경우 주기성이나 계절성 같이 일정한 패턴을 가지는 자기상관성이 발생함

- 시간에 따라 평균이 변하는 경우: ARMA(Auto Regressive Moving Average) Model

- 시간에 따라 분산이 달라지는 경우: ARCH(Auto Regressive Conditionally Heteroscedastic) Model

### (4) 정규성(Normality) 가정: 오차항의 확률분포가 정규분포인지 파악 -> X -> 일반화선형모형(GLM): 로지스틱회귀 / 서열회귀 / 포아송회귀 / 생존회귀 

- 산출/결과/반응/종속변수의 분포가 정규분포인지 확인

### (5) 종속변수가 정규분포가 아닐경우 오차항의 확률분포도 정규분포가 아닌 경우

##### 일반화선형모형(Generalized Linear Model)

- 종속변수가 0 또는 1인경우: 로지스틱회귀(Logistic regression)

● 분포모양이 정규분포가 아니기 때문에 확률적으로 다른 기법 마련되어 있다.

- 종속변수가 순위, 선호도 같이 서열인경우: 서열회귀(Ordinal regression)

● 종속변수가 서열척도 일 경우.

- 종속변수가 개수(count)를 나타내는 경우: 포아송회귀(Poisson regression)

● 종속변수가 counting 된 경우. 앞쪽에 많이 발생한 경우 대응하는 기법.

- 종속변수가 사망시간이나 중도절단(censoring) 데이터인 경우: 생존회귀(Survival regression)

● 종속변수가 시간대에 따라 멈추는 기간이 존재하는 경우. 


### (6) 등분산성(Equality of Variance/ Homoscedasticity) -> X -> log 변환 / 가중최소제곱법 적용

- 잔차의 분산이 평균 0 주위에 일정한 패턴없이 분포하는지 확인

### (7) 등분산성에 위배되었을 때 

- log 변환, 가중최소제곱법 적용


### (8) 이상치와 영향치 -> O -> 로버스트회귀 / 분위값회귀

● 이상치는 아웃라이어. 오차가 많이 발생하는 데이터. 지우면 됨. 

● 영향치는 예측선 주위에 있는데, 경계선에 걸쳐있는 값들이며, 드문드문 있어서 한 두개 의 데이터가 예측선에 영향 미친 걸로, 과다 추정을 의심받는 값. 얘를 보완하면 더 좋은 예측선 나올 수 있다는 취지. 무조건 지우면 안 되고 그 구간의 레코드 많이 만들어서 대표성 만들면 좋음.

### (9) 데이터에 아웃라이어가 있는 경우

##### 로버스트회귀(Robust regression)

- 잔차의 제곱을 이용하는 최소제곱법 대신에 절대값의 합이 최소가 되도록 계수룰 추정하는 방식

##### 분위값회귀(Quantile regression)

- 평균이 아닌 특정 분위값을 추정해서 그 위치에 있는 종속변수값을 사용해 아웃라이어의 영향을 해소


### (10) 다중공선성이 있는 경우 -> O -> 패털티 함수 추가 (Ridge / Lasso / Elastic net) / 데이터 축소 방식 (PCT / PLS)

- 고전적인 선형회귀모델을 회귀계수 추정을 위해 잔차의 제곱합을 계산하는 일종의 비용함수를 만들어서 비용함수가 최소가 되는 회귀계수를 추정하게 됨

- 다중공선성이 있는 독립변수들이 투입된 상태의 비용함수에서는 회귀계수의 영향력이 과다추정될 수 있음

- 고전적인 선형회귀모델의 회귀계수 추정시 사용하는 잔차의 제곱의 합을 계산하는 비용함수에 페널티(regularization) 함수라는 추가적인 수식을 붙여 회귀계수값들의 과다추정을 막고, 오버피팅도 방지함

- 이 기존의 잔차제곱합을 계산하는 비용함수에 페널티를 어떻게 줄 것인지에 따라 여러 회귀분석으로 나누어짐

- 데이터의 개수에 비해 독립변수의 개수가 많을 때도 이런방식을 사용할 수 있음

● 예측선 구하는 걸 비용함수라고 표현하기도 함. 


##### 페널티(regularization) 함수 추가방식

###### **** 릿지회귀(Ridge regression)

- 잔차의 제곱합을 계산하는 방식 --> 회귀계수의 제곱합을 계산하는 방식 추가

###### **** 라쏘회귀(Lasso regression)

- 잔차의 제곱합을 계산하는 방식 --> 회귀계수의 절대값을을 계산하는 방식 추가

- 영향력이 적은 변수의 회귀계수값을 0으로 만들어 주는 변수선택효과도 있음

###### **** 엘라스틱넷회귀(Elastic net regression)

- 릿지회귀와 라쏘회귀를 결합한 방식

##### 데이터 축소(Data reduction) 방식

###### **** PCR(Principal Component Regression)

- 독립변수들의 주성분을 추출/이용해 회귀모델을 만듬

- 주성분들이 서로 직교하므로 다중공선성 발생하지 않음

- 상위 몇 개 주성분만 이용할 경우 라쏘처럼 일종의 regualization 효과를 발생시켜 모델이 오버피팅 현상도 완화됨

- 그러나 모델 해석은 어려울 수 있음

● 서로 상관성 높은 변수끼리 모으는 것

###### **** PLS(Partial Least Square) regression

- PCR과 비교했을 때 변수변환 방식에서 차이가 남

- PCR: 독립변수의 분산을 최대로 하는 축을 찾아 데이터를 전사(projection)하는 방식으로 독립변수만 변형함

- PLS: 종속변수와 독립변수의 관계를 가장 잘 설명해주는 축을 찾아 전사하는 방식으로 종속변수와 독립변수 모두를 변형함

<br>

---------------코딩------------------------------------------------------------------------------------------------------

<br>

##### 비즈니스 시나리오

- 카드사 이용고객 중 쿠폰이벤트를 진행했을 때 다양한 반응금액이 나옴
- 이용고객들의 어떤 특성들이 작용해 반응금액을 만들어 낼까? 
- 반응 금액을 결정하는 조건/요인/속성/변수/기준/피처(feature)와 이들이 가지는 임계치와 상대적인 중요도는?



##### 분석 모델링 및 조작적 정의

- avrprice(1회평균 카드사용금액) --> 수치데이터(만원)
- period(1회평균 온라인쇼핑몰 접속시간) --> 수치데이터(시간)
- variety(상품구매다양성) --> 수치데이터(부문)
- job(직업유형) --> 범주형데이터(1:student, 2:officeworker: 3:housewife)
- age(연령) --> 수치데이터(세)
- total(이벤트 반응금액) --> 수치데이터(만원) 




<br>

<a id="1"></a>

## 1. 데이터셋 로딩

<br>

<a id="1_1"></a>

### (1) 데이터셋 로딩

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="1_2"></a>

### (2) 데이터셋 간단조회: head() / str()

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2"></a>

## 2. 데이터탐색

<br>

<a id="2_1"></a>

### (1) 기술통계: summary()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_1"></a>

### (2) 산점도: plot() / PerformanceAnalytics::chart.Correlation()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_3"></a>

### (3) 선형성 가정: plot() / crPlots()

<br>

● 만약 위의 빨간 선이 꺾임이 있으면 잔차가 어떤 패턴이 있다는 것.



● 만약 그런 패턴이 안나오면 예측선 찾기 위해 아래의 기법들이 나왔다. 

● 다항회귀 등은 예측이 구간 별로 있기 때문에 곡선으로 예측선을 만든다. 

● 독립변수를 제곱, 세제곱 등을 해서 그래프가 곡선이 나온다. 


<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_4"></a>

### (4) 독립성 가정: durbinWatsonTest()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_5"></a>

### (5) 정규성 가정: plot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_6"></a>

### (6) 등분산성 가정: plot() / ncvTest() / spreadLevelPlot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_7"></a>

### (7) 영향 관측치와 이상치 파악: plot() / car::outlierTest() / influencePlot()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="2_8"></a>

### (8) 다중공선성 가정: vif() > 10

<br>

● 통상 10이 넘어가면 독립변수 간 그런 성질이 있다고 간주. 과다추정이나 과소추정될 가능성 있다고 판단.

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3"></a>

## 3. 데이터 전처리

<br>

<a id="3_1"></a>

### (1) 종속변수 지정

<br>

● 독립변수에 범주 데이터 있으면 dummy 변수라고 한다. 원래는 연속 데이터만 넣어야하는데 그럴 수만은 없어서 범주 데이터도 넣는다. 이게 왜 중요하냐면 직업이 1, 2, 3, 직업이 있을 때, 직업의 귀천으로 인식하지 말 고단순히 항목을 구분하는 거니 가치 중립적이며 1일 때 함수 따로, 2, 3일 때 딸 만들어야 함. (원핫인코딩)

● 만약 독립변수를 숫자 항목으로 구분하지 못하면 factor( job ) 으로 먼저 뽑아줘야 한다. 

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_2"></a>

### (2) 독립변수 중 범주형 변수컬럼 지정

<br>

<br>
<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_3"></a>

### (3) 독립변수 중 연속형 변수컬럼 지정: setdiff()

<br>

<br>
<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_4"></a>

### (4) 독립변수 중 범주형 변수컬럼을 일괄팩터화하는 사용자 정의함수 및 일괄팩터화

<br>

<br>
<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_5"></a>

### (5) 독립변수 중 연속형 변수컬럼을 일괄스케일링하는 사용자 정의함수 및 일괄스케일링

<br>

<br>
<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="3_6"></a>

### (6) 전체 데이터를 6:2:2로 분할: 학습, 검증, 테스트

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4"></a>

## 4. 훈련데이터 다중회귀분석

<br>

<a id="4_1"></a>

### (1) 훈련데이터로 다중회귀모델 설정: stats::lm()

<br>

● lm: linear model. 선형 관계를 찾는 함수. x를 어느 정도 움직이면 y가 어느 정도 움직이는지 예측. y = ax+b 함수 로직이 여기 들어가 있다. 

● a는 기울기인데, 회귀계수, 결정계수라 부르며, 독립변수의 움직임을 대표할 수 있는 기준이 되는 숫자이며, y를 결정하는 것.

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4_2"></a>

### (2) 회귀분석결과 확인: summary() / confint() / signif() / coef()

<br>

● intersept: 상수항.

● residuals: 잔차의 범위. 그 범위가 이 정도 된다는 걸 식으로 해서 결과 보여줌

● pvalue; 어떤 변수가 중요한 절댓같인지. 

● 결정계수: 설명력. 0에서 1사이 인데, 1로 갈수록 예측력이 우수하다, 설명력이 우수하다. 

● standard error: 표준오차.

● R-squared도 숫자가 높으면 좋은데, 특별한 기준이 존재하지는 않는다. R-squared 값이 조금이라도 높으면 예측력이 높다. 독립변수가 많을수록 이 값도 높아진다. (예를 들면, 신입사원 채용 시 평가 측정 항목이 많을 수록 좋은 사원 뽑을 확률 높아지는 것처럼.)

● R-squared는 85% 정도 커버하는 예측식을 찾았다는 의미. 얘를 높이기 위해 의미 없는 변수 없애기, 그룹핑 등등 작업 한다. 







● 95% 신뢰구간 내에서 각 변수별 회귀계수(기울기)의 신뢰구간 보여줌.

● 보수적으로는 2.5% 열의 기울기 선택, 좀 여유두면 97.5% 열의 기울기 선택.

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4_3"></a>

### (3) 회귀예측모델 독립변수 선택 방법: stats::step() / MASS::stepAIC()

<br>

● 변수를 추가할수록 데이터가 많아지긴 해서 어느 선까지는 설명력이 높아지는데, 한계가 있다. 그 때까지 변수를 넣었다 뺐다 하면서 설명력을 높이면 된다. 이후로는 오차 같은 것도 많아져서 설명력 더이상 안 높아진다. 이 넣었다 뺐다 하는 걸 자동으로 해주는 식이 step 함수

● step 함수, direction = "forward" : fit 데이터 가지고 여기에 든 독립 변수 가지고 변수를 하나씩 추가해달라, 더이상 R-squared값이 증가하지 않을 때까지. 



● direction = "backward" : 전체 변수를 한꺼번에 넣고 의미 없는 변수들을 하나씩 빼는 작업.



● 모형자체에 변수 넣었다 뺐다 하는 걸 여러가지로 할 수 있다. forward와 backward는 단방향인데, direction = "both"는 넣었다가 빼기도 하고 양방향으로 진행되는 방식. 

● AIC: 적을수록 좋은 값.

<br>
<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4_4"></a>

### (4) 회귀분석에 사용된 투입변수 중요도 계산: varImp() / ggplot()

<br>

<br>
<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="4_5"></a>

### (5) 훈련데이터로 학습한 모델간 평가: AIC() / BIC() / summary()$adj.r.squared

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="5"></a>

## 5. 검증데이터&테스트데이터 수치 예측

<br>

<a id="5_1"></a>

### (1) 데이터 예측: predict(모델, 데이터, interval = "")

<br>

● 새 데이터를 데이터프레임 형식으로 만들어주는 게 중요함.

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="5_2"></a>

### (2) 검증데이터 대상 회귀모델간 수치예측 성능평가: caret::defaultSummary() / Metrics::rmse() / Metrics::mae()

<br>

● predict 함수: 예측하는 함수. fit이라는 메모리 방이 있다. 거기 찾아가면 lm 함수로 선형회귀로 total 값 예측하는 규칙하나 있다. 독립변수만 있어 total값은 모르는 new1이란 새 데이터가 있으니 여기에 적용해달라.

● interval = "none": 구간 추정하지 말고, 점추정해달라.

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="5_3"></a>

## (3) 테스트셋을 대상으로 최종적으로 모델간 성능평가: 예측 및 성능평가

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="6"></a>

## 6. 예측 결과 정리

<br>

<a id="6_1"></a>

### (1) 예측결과 결합: cbind()

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="6_2"></a>

### (2) 예측결과 내림차순 정리: decreasing = T

<br>

<br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="6_3"></a>

### (3) 예측값 필터링: 

<br>

<br><br>

<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<a id="7"></a>

## 7. 회귀분석 모델 진단 그래프: 멀티캔버스 / 개별 출력



<!-- ------------------------------------------------------------------------------------------------------------------------------- -->

<br><br>


###### <참고 문헌> 

<br>

1. 최점기 박사님 강의 <br>

